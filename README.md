# Transformer Implementation

My implementation of the original [Transformer](https://arxiv.org/abs/1706.03762), trained on Multi30k dataset for German to English translation. 

I haven't done any evaluations yet, but got this result from one epoch of training through the whole dataset on my laptop: 

![Predicted (top) vs Ground Truth](sample_inference.png)

Room for improvement for sure !! More changes to come

Note: the cleaned up notebook is in [bpe_tokenized_transformer](https://github.com/ishanshastri/Transformers/blob/main/bpe_tokenized_transformer.ipynb); I've included my 'rough work' in [bpe_tokenized_transformer_rough](https://github.com/ishanshastri/Transformers/blob/main/bpe_tokenized_transformer_rough.ipynb)
