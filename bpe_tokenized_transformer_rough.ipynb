{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishanshastri/Library/Python/3.9/lib/python/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/ishanshastri/Library/Python/3.9/lib/python/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import einops\n",
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import huggingface char-bpe tokenizer\n",
    "en_tokenizer, de_tokenizer = CharBPETokenizer(), CharBPETokenizer()\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_tokenizer.add_special_tokens(special_symbols)\n",
    "de_tokenizer.add_special_tokens(special_symbols)\n",
    "\n",
    "# Train tokenizers\n",
    "train_iter, test_iter, valid_iter = Multi30k(language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "\n",
    "de_data, en_data = (list(zip(*train_iter)))\n",
    "en_tokenizer.train_from_iterator(iterator=en_data)\n",
    "de_tokenizer.train_from_iterator(iterator=de_data)\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = tuple(en_tokenizer.encode(x).ids[0] for x in special_symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.encode(en_data[0])#.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform_en = lambda x : torch.tensor([BOS_IDX] + en_tokenizer.encode(x.rstrip(\"\\n\")).ids + [EOS_IDX])\n",
    "text_transform_de = lambda x : torch.tensor([BOS_IDX] + de_tokenizer.encode(x.rstrip(\"\\n\")).ids + [EOS_IDX])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(list(train_iter), dtype=torch.StringType)\n",
    "# type(train_iter)\n",
    "# (list(zip(*train_iter)))[0][9]\n",
    "# en_data, de_data = (list(zip(*train_iter)))\n",
    "# de_data_tokenized, en_data_tokenized = \n",
    "# map(lambda x : (x[0], x[1]), train_iter)#list(map(text_transform_de, de_data)), list(map(text_transform_de, de_data))\n",
    "# # de_data_tokenized\n",
    "# tokenized_multik3 = list(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokenized_multik3), len(list(train_iter)), type(train_iter)\n",
    "# torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(tokenized_multik3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), train_iter)))\n",
    "tokenized_train_iter = torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), train_iter))\n",
    "tokenized_test_iter = torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), test_iter))\n",
    "tokenized_valid_iter = torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), valid_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(tokenized_train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Position encoding\"\"\"\n",
    "class PositionEncoding2(nn.Module):\n",
    "    def __init__(self, d_embed):\n",
    "        super(PositionEncoding2, self).__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def get_position_encoding(self, seq_len):\n",
    "        encoding = torch.zeros((seq_len, self.d_embed))\n",
    "        dimensions = torch.arange(0, self.d_embed//2)\n",
    "        timesteps = torch.arange(0, seq_len)\n",
    "\n",
    "        encoding[:, 0::2] = torch.sin(torch.einsum('i,j -> ji', 1/(10000**(2*dimensions/self.d_embed)), timesteps))\n",
    "        encoding[:, 1::2] = torch.cos(torch.einsum('i,j -> ji', 1/(10000**(2*dimensions/self.d_embed)), timesteps))\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        pos_encoding = nn.Parameter(self.get_position_encoding(seq_len=inp.shape[-2]), requires_grad=False)\n",
    "        return self.dropout(inp + pos_encoding) # + dropout ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multihead attention\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, n_heads):\n",
    "        super().__init__()#MultiheadAttention)\n",
    "        self.d_hidden=d_hidden\n",
    "        self.n_heads = n_heads\n",
    "        self.W_q = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden//n_heads)))\n",
    "        )\n",
    "        self.W_k = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden//n_heads)))\n",
    "        )\n",
    "        self.W_v = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden//n_heads)))\n",
    "        )\n",
    "\n",
    "        self.W_o = nn.Linear(in_features=d_hidden, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v, get_attn_scores=False, mask=None, is_causal=False):\n",
    "        mask_shape = (q.shape[-2], k.shape[-2]) \n",
    "        # print(\"mask shape: \", mask_shape)\n",
    "        mask = mask if mask else (self.causal_mask(self.n_heads, mask_shape) if is_causal else self.empty_mask(self.n_heads, mask_shape))\n",
    "\n",
    "        # Compute Q, K, V (matrix multiplication, batched along heads)\n",
    "        q = einops.einsum(self.W_q, q, 'h d_model d_h, b T d_model -> b h  T d_h') # T is timesteps (sequence length)\n",
    "        k = einops.einsum(self.W_k, k, 'h d_model d_h, b T d_model -> b h  T d_h')\n",
    "        v = einops.einsum(self.W_v, v, 'h d_model d_h, b T d_model -> b h  T d_h')\n",
    "\n",
    "        # print(\"q_shape: \", q.shape)\n",
    "        # print(\"k_shape: \", k.shape)\n",
    "\n",
    "        attn_scores = torch.softmax(\n",
    "            einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(self.d_hidden//self.n_heads) + mask,#torch.FloatTensor(self.d_hidden//8)),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        attn_head_outs = einops.einsum(attn_scores, v, '... T_out T_in, ... T_in d_h -> ... T_out d_h')\n",
    "        # print(attn_head_outs.shape)\n",
    "        # print(attn_scores, \"mask: \", mask)\n",
    "        # print(\"mask: \", mask)\n",
    "        concatted_outs = einops.rearrange(attn_head_outs, 'b h T_out d_h -> b T_out (h d_h)')\n",
    "        concatted_outs = self.W_o(concatted_outs)\n",
    "\n",
    "        return (concatted_outs, attn_scores) if get_attn_scores else concatted_outs\n",
    "    \n",
    "    @classmethod\n",
    "    def causal_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.triu(\n",
    "                torch.fill(torch.zeros(mask_shape),  -torch.inf),\n",
    "                diagonal=1\n",
    "                )\n",
    "            , pattern='... -> k ...', k=n_heads\n",
    "            )\n",
    "    \n",
    "    @classmethod\n",
    "    def empty_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.zeros(mask_shape) , pattern='... -> k ...', k=n_heads\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multihead attention\"\"\"\n",
    "class MultiHeadAttention2(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, n_heads):\n",
    "        super().__init__()#MultiheadAttention)\n",
    "        self.d_hidden=d_hidden\n",
    "        self.n_heads = n_heads\n",
    "        # self.W_q = nn.Parameter(\n",
    "        #     nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden)))\n",
    "        # )\n",
    "        # self.W_k = nn.Parameter(\n",
    "        #     nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden)))\n",
    "        # )\n",
    "        # self.W_v = nn.Parameter(\n",
    "        #     nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden)))\n",
    "        # )\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_hidden)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_hidden)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_hidden)\n",
    "\n",
    "        self.W_o = nn.Linear(in_features=d_hidden, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v, get_attn_scores=False, mask=None, is_causal=False):\n",
    "        mask_shape = (q.shape[-2], k.shape[-2]) \n",
    "        # print(\"mask shape: \", mask_shape)\n",
    "        mask = mask if mask else (self.causal_mask(self.n_heads, mask_shape) if is_causal else self.empty_mask(self.n_heads, mask_shape))\n",
    "        # print(\"in q\", k.shape)\n",
    "        # Compute Q, K, V (matrix multiplication, batched along heads)\n",
    "        q = self.W_q(q)#einops.einsum(self.W_q, q, 'h d_model d_h, b T d_model -> b h T d_h') # T is timesteps (sequence length)\n",
    "        k = self.W_k(k)#einops.einsum(self.W_k, k, 'h d_model d_h, b T d_model -> b h T d_h')\n",
    "        v = self.W_v(v)#einops.einsum(self.W_v, v, 'h d_model d_h, b T d_model -> b h T d_h')\n",
    "        # print(\"out1 q\", k.shape)\n",
    "        # q = einops.rearrange(q, 'b T d_h -> b T h k', h=self.n_heads)\n",
    "        q = einops.rearrange(q, 'b T (h w) -> b h T w', h=self.n_heads)\n",
    "        k = einops.rearrange(k, 'b T (h w) -> b h T w', h=self.n_heads)\n",
    "        v = einops.rearrange(v, 'b T (h w) -> b h T w', h=self.n_heads)\n",
    "        # print(\"q_shape: \", q.shape)\n",
    "        # print(\"k_shape: \", k.shape)\n",
    "        # print(\"out2 q\", k.shape)\n",
    "\n",
    "        attn_scores = torch.softmax(\n",
    "            einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(self.d_hidden//self.n_heads) + mask,#torch.FloatTensor(self.d_hidden//8)),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        attn_head_outs = einops.einsum(attn_scores, v, '... T_out T_in, ... T_in d_h -> ... T_out d_h')\n",
    "        # print(attn_head_outs.shape)\n",
    "        # print(attn_scores, \"mask: \", mask)\n",
    "        # print(\"mask: \", mask)\n",
    "        concatted_outs = einops.rearrange(attn_head_outs, 'b h T_out d_h -> b T_out (h d_h)')\n",
    "        concatted_outs = self.W_o(concatted_outs)\n",
    "\n",
    "        return (concatted_outs, attn_scores) if get_attn_scores else concatted_outs\n",
    "    \n",
    "    @classmethod\n",
    "    def causal_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.triu(\n",
    "                torch.fill(torch.zeros(mask_shape),  -torch.inf),\n",
    "                diagonal=1\n",
    "                )\n",
    "            , pattern='... -> k ...', k=n_heads\n",
    "            )\n",
    "    \n",
    "    @classmethod\n",
    "    def empty_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.zeros(mask_shape) , pattern='... -> k ...', k=n_heads\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "         [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "         [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]]],\n",
       "\n",
       "\n",
       "        [[[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "         [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "\n",
       "         [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "          [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, h, T_out, d_h = 2, 3, 4, 6\n",
    "q = torch.ones((b, h, T_out, d_h)) # target seq\n",
    "k = torch.ones((b, h, T_out+2, d_h)) # inp seq\n",
    "\n",
    "mask = MultiHeadAttention2.empty_mask(n_heads=h, mask_shape=(q.shape[-2], k.shape[-2]))#(k.shape[-2], q.shape[-2]))\n",
    "# mask = \n",
    "# print(mask)\n",
    "attn_scores = torch.softmax(\n",
    "            einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(2) + mask,#torch.FloatTensor(self.d_hidden//8)),\n",
    "            dim=-1\n",
    "        )\n",
    "attn_scores#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0892,  0.3523,  0.3153, -1.1927,  0.8217, -0.6601],\n",
      "         [-0.5424,  0.5256, -0.4009, -0.9617,  0.7779, -0.9696],\n",
      "         [-0.1952,  1.3644, -0.2244, -1.2910,  0.5016, -0.7282],\n",
      "         [-0.2089,  0.8486,  0.0332, -1.5708,  0.7290, -1.0882]],\n",
      "\n",
      "        [[ 0.2547,  0.8348, -0.1010, -0.2742, -0.4685,  0.2282],\n",
      "         [-0.1984,  1.0087, -0.8172, -0.0431, -0.5128, -0.0809],\n",
      "         [-0.1952,  1.3644, -0.2244, -1.2910,  0.5016, -0.7282],\n",
      "         [-0.1173,  1.1746, -0.0354, -1.3611,  0.2980, -0.7629]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0892,  0.3523],\n",
      "          [-0.5424,  0.5256],\n",
      "          [-0.1952,  1.3644],\n",
      "          [-0.2089,  0.8486]],\n",
      "\n",
      "         [[ 0.3153, -1.1927],\n",
      "          [-0.4009, -0.9617],\n",
      "          [-0.2244, -1.2910],\n",
      "          [ 0.0332, -1.5708]],\n",
      "\n",
      "         [[ 0.8217, -0.6601],\n",
      "          [ 0.7779, -0.9696],\n",
      "          [ 0.5016, -0.7282],\n",
      "          [ 0.7290, -1.0882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2547,  0.8348],\n",
      "          [-0.1984,  1.0087],\n",
      "          [-0.1952,  1.3644],\n",
      "          [-0.1173,  1.1746]],\n",
      "\n",
      "         [[-0.1010, -0.2742],\n",
      "          [-0.8172, -0.0431],\n",
      "          [-0.2244, -1.2910],\n",
      "          [-0.0354, -1.3611]],\n",
      "\n",
      "         [[-0.4685,  0.2282],\n",
      "          [-0.5128, -0.0809],\n",
      "          [ 0.5016, -0.7282],\n",
      "          [ 0.2980, -0.7629]]]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "q.shape\n",
    "q = torch.ones((b, T_out, d_h))\n",
    "s = MultiHeadAttention2(d_model=d_h, d_hidden=d_h, n_heads=h)\n",
    "ti = PositionEncoding2(d_embed=d_h)\n",
    "q = ti(q)\n",
    "q = s.W_q(q)\n",
    "q.shape\n",
    "print(q)\n",
    "q = einops.rearrange(q, 'b T (h w) -> b h T w', h=h)#, w=d_h//h)\n",
    "q.shape, q\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048):\n",
    "        super(PosWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FFN(x) = max(0, xW1 + b1)W2 + b2 ; f(x) = max(0, x) is relu\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, num_heads, d_ff, dropout=0.1, N_layers=6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, d_hidden=d_hidden, n_heads=num_heads)\n",
    "        self.feed_forward = PosWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None, is_causal=False):\n",
    "        #print(\"Q, K, V: \", x.size())\n",
    "        attn_output = self.self_attn(x, x, x, mask=mask, is_causal=is_causal) # self attention\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # layer-norm + dropout + skip connection\n",
    "        ff_output = self.feed_forward(x) # feed-forward\n",
    "        x = self.norm2(x + self.dropout(ff_output)) # layer-norm + dropout + skip connection\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, num_heads, d_ff, dropout=0.1, N_layers=6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_hidden, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, d_hidden, num_heads)\n",
    "        self.feed_forward = PosWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask, is_causal=False):\n",
    "        attn_output = self.self_attn(x, x, x, mask=tgt_mask, is_causal=is_causal) # self attention\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # layer-norm + dropout + skip connection\n",
    "        attn_output = self.cross_attn(q=x, k=enc_output, v=enc_output, mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output)) # layer-norm + droput + skip connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output)) # layer-norm + dropout + skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, num_heads, d_ff,src_vocab_size, tgt_vocab_size, dropout=0, n_enc_layers=6, n_dec_layers=6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model=d_model,\n",
    "                         d_hidden=d_hidden,\n",
    "                         num_heads=num_heads,\n",
    "                         d_ff=d_ff) for _ in range(n_enc_layers)]\n",
    "        )\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model=d_model,\n",
    "                         d_hidden=d_hidden,\n",
    "                         d_ff=d_ff,\n",
    "                         num_heads=num_heads) for _ in range(n_dec_layers)]\n",
    "        )\n",
    "\n",
    "        self.pos_encoding_layer = PositionEncoding2(d_embed=d_model)\n",
    "\n",
    "        self.src_embedding = nn.Embedding(embedding_dim=d_model, num_embeddings=src_vocab_size, padding_idx=PAD_IDX)\n",
    "        self.tgt_embedding = nn.Embedding(embedding_dim=d_model, num_embeddings=tgt_vocab_size, padding_idx=PAD_IDX)\n",
    "\n",
    "        self.final = nn.Linear(in_features=d_hidden, out_features=tgt_vocab_size)\n",
    "\n",
    "    def forward(self, inp_seq, tgt_seq, inp_mask=None, tgt_mask=None, is_causal_tgt=False):\n",
    "        \n",
    "\n",
    "        inp_seq = self.src_embedding(inp_seq)\n",
    "        inp_seq = self.pos_encoding_layer(inp_seq)\n",
    "\n",
    "        for enc in self.enc_layers:\n",
    "            inp_seq = enc(inp_seq, mask=inp_mask, is_causal=False)\n",
    "\n",
    "        # inp_seq = self.enc_layers(inp_seq, mask=inp_mask, is_causal=False)\n",
    "\n",
    "        tgt_seq = self.src_embedding(tgt_seq)\n",
    "        # tgt_seq = self.dec_layers(x=tgt_seq, enc_out=inp_seq, mask=tgt_mask, is_causal=is_causal_tgt)\n",
    "        for dec in self.dec_layers:\n",
    "            tgt_seq = dec(x=tgt_seq, enc_output=inp_seq, src_mask=inp_mask, tgt_mask=tgt_mask, is_causal=is_causal_tgt)\n",
    "\n",
    "        out_logits = self.final(tgt_seq)\n",
    "        return out_logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[[ 0.4632, -0.7032, -0.8027,  0.1558, -0.2824, -0.8684, -0.4869],\n",
       "          [ 1.1471, -0.0685, -0.1344,  0.0605, -0.5780, -0.6808, -0.3462],\n",
       "          [ 1.3599, -0.2954, -0.0264,  0.1407, -0.1240, -0.5058, -0.4269]],\n",
       " \n",
       "         [[ 0.2772, -0.9365, -0.7626, -0.4455, -0.2436, -0.6227, -0.7306],\n",
       "          [ 1.0347, -0.5363, -0.3671, -0.3951, -0.3378, -0.9007, -0.5157],\n",
       "          [ 1.0146, -0.5092, -0.3398,  0.2083, -0.4329, -0.6571, -0.3672]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Transformer(d_model=8, d_hidden=8, num_heads=4, d_ff=10, src_vocab_size=3, tgt_vocab_size=7)\n",
    "src, tgt = torch.arange(0, 3).tile(2, 1), torch.arange(0, 3).tile(2, 1)\n",
    "\n",
    "src.shape, t(src, tgt)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # print(batch)\n",
    "    src_batch, tgt_batch = list(zip(*batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX).transpose(-1, -2)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX).transpose(-1, -2)\n",
    "\n",
    "    return src_batch, tgt_batch \n",
    "\n",
    "p = list(zip(*tokenized_test_iter))\n",
    "# list(tokenized_test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_tokenizer.decode((pad_sequence(p[0], padding_value=PAD_IDX).transpose(-1, -2))[1013].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_batch = list(test_iter)#train_iter#list(train_iter)\n",
    "# collated_samp = collate_fn(tokenized_test_iter)\n",
    "# uncolled = list(tokenized_test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collated_samp[0][len(collated_samp[0])-3]\n",
    "# uncolled = list(tokenized_test_iter)\n",
    "# l_iter = list(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(uncolled), len(collated_samp[0]) #(1015, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncolled[0], collated_samp[0][0], torch.max(collated_samp[0][len(collated_samp[0])-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   2,  313,  400,  826, 1276,    3]),\n",
       " tensor([   2, 1859, 2613, 5024, 3934,    3]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple task to overfit the model on; helps to catch dumb bugs in the architecture and training\n",
    "text_transform_en('one two three four'), text_transform_en('five six seven eight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "D_MODEL = 512\n",
    "D_FF = 2048\n",
    "N_HEADS = 8\n",
    "\n",
    "BATCH_SIZE = 1#64\n",
    "DEVICE = 'cpu'\n",
    "LR = 0.00001#math.sqrt(D_MODEL/4000) # make this a schedule\n",
    "BETAS = (0.9, 0.98)\n",
    "\n",
    "# train_dataloader = DataLoader(tokenized_train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "# test_dataloader = DataLoader(tokenized_test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "# valid_dataloader = DataLoader(tokenized_valid_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, data_loader=None):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    tokenized_train_iter = torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), train_iter))\n",
    "    train_dataloader = DataLoader(tokenized_train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    data_count = 0\n",
    "    import tqdm\n",
    "    for src, tgt in tqdm.tqdm(train_dataloader):#data_loader):#train_dataloader):\n",
    "        data_count+=1\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        out_logits = model(src, tgt[:, :-1], is_causal_tgt=True)\n",
    "\n",
    "        # print(\"inputs: \", de_tokenizer.decode(list(src.squeeze()), skip_special_tokens=False), en_tokenizer.decode(list(tgt[:, :-1].squeeze()), skip_special_tokens=False))\n",
    "\n",
    "\n",
    "        # print(\"out_logits shape: \", out_logits.shape)\n",
    "        # print(\"tgt shape: \", tgt.shape) b seq vcb\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(\"out_logits: \", out_logits.transpose(-1, -2), out_logits.transpose(-1, -2).shape)\n",
    "        # print(\"tgt: \", tgt )\n",
    "        \n",
    "\n",
    "        loss = loss_fn(out_logits.transpose(-1, -2), tgt[:, 1:])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    print(\"Trained on %s datapts\"%data_count)\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    data_count=0\n",
    "    for src, tgt in val_dataloader:\n",
    "        data_count+=1\n",
    "        if data_count==50:\n",
    "            break\n",
    "        src = src.to(DEVICE).T\n",
    "        tgt = tgt.to(DEVICE).T\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        print(\"src\", src.T.shape)\n",
    "        print(\"tgt\", tgt.T.shape)\n",
    "\n",
    "        # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        output = model(src, tgt[:,:-1])\n",
    "\n",
    "        \n",
    "\n",
    "        loss = loss_fn(output.transpose(-1, -2), tgt)\n",
    "        # loss = criterion(output.contiguous().view(-1, TGT_VOCAB_SIZE), tgt[:, 1:].contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Transformer(d_model=D_MODEL, \n",
    "                    d_hidden=D_MODEL, \n",
    "                    num_heads=N_HEADS, \n",
    "                    d_ff=D_FF, \n",
    "                    src_vocab_size=de_tokenizer.get_vocab_size(), \n",
    "                    tgt_vocab_size=en_tokenizer.get_vocab_size()\n",
    "                    )\n",
    "\n",
    "optimizer = torch.optim.Adam(betas=BETAS, eps=10e-9, lr=LR, params=model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/ishanshastri/Library/Python/3.9/lib/python/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "29001it [1:26:40,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on 29001 datapts\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, train_dataloader)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m      9\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\u001b[38;5;66;03m#evaluate(model)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[129], line 57\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, data_loader)\u001b[0m\n\u001b[1;32m     53\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m datapts\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mdata_count)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlosses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# de_tokenizer.get_vocab_size()\n",
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)#, train_dataloader)\n",
    "    end_time = timer()\n",
    "    val_loss = 0#evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'oct2+1')\n",
    "# model = torch.load('oct2+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, mask = torch.ones((32, 8, 23, 64)), torch.ones((32, 8, 24, 64)), torch.ones((23, 24))\n",
    "attn_scores = torch.softmax(\n",
    "            einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(2) + mask,#torch.FloatTensor(self.d_hidden//8)),\n",
    "            dim=-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 23, 24])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(2)).shape# + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_toks = [BOS_IDX]\n",
    "random_sample = 80\n",
    "src_toks = de_tokenizer.encode(de_data[random_sample]).ids\n",
    "model.eval()\n",
    "toks_generated=0\n",
    "max_len=100\n",
    "while y_toks[-1] != EOS_IDX and toks_generated < max_len:\n",
    "    toks_generated+=1\n",
    "    out_logits = model(torch.tensor(src_toks).unsqueeze(0), torch.tensor(y_toks).unsqueeze(0), is_causal_tgt=True)\n",
    "    # print(out_logits)\n",
    "    # print(out_logits[:,-1].shape, \"sfdsfhadsfhsdkjhfSDFfdf\")\n",
    "    idx_predicted = torch.argmax(out_logits[:,-1])\n",
    "    y_toks.append(idx_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2,\n",
       "  tensor(218),\n",
       "  tensor(384),\n",
       "  tensor(559),\n",
       "  tensor(201),\n",
       "  tensor(287),\n",
       "  tensor(170),\n",
       "  tensor(97),\n",
       "  tensor(487),\n",
       "  tensor(287),\n",
       "  tensor(170),\n",
       "  tensor(97),\n",
       "  tensor(487),\n",
       "  tensor(149),\n",
       "  tensor(3)],\n",
       " 'Two large dogs are playing on a beach playing on a beach .',\n",
       " 'Two large tan dogs play along a sandy beach.')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_toks,\n",
    "en_tokenizer.decode(y_toks),#[2456]),\n",
    "en_data[random_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'epoch_1_multiheadattention1_translator_de_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros((2, 2, 3, 3)) + MultiHeadAttention.causal_mask(n_heads=2, mask_shape=(3,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
